import sys

import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
from nltk.stem.snowball import SnowballStemmer
from nltk.stem.porter import *
from textblob import Word
from textblob import TextBlob

import spacy

# ========================================================================
# txtParser: методами строк очистить текст от знаков препинания;
# привести все слова к нижнему регистру (map);
# сформировать list со словами (split);
class txtParser:     # the program for parsing

    # список нежелательных символов (знаки препинания, скобки и т.п.)
    badSignes = ['.', ',', ';', ':', '(', ')', '«', '»']
    #                        значение по умолчанию
    def __init__(self, fnameIn, substrLen = 90):
        self.fileIn = open(fnameIn, 'r', encoding='utf16')
        self.txtArr = self.fileIn.readlines()   # прочитать ВСЕ строки в файле в буфер (массив)
                                                # txtArr, который состоит из len(txtArr) строк,
                                                # считая и пустые, которые содержат символ '\n'
                                                # и больше ничего

        self.strings = ''          # рабочий буфер приложения
        self.stringsLower = ''     # буфер для приведения слов к нижнему регистру в мфссив (map)
        self.stringsLowerTxt = ''  # буфер для приведения слов к нижнему регистру в текст
        self.wordsArray = []       # список слов
        self.uniqueWords = []      # список всех слов без повторения
        self.baseWords = []        # список для определения количества разных слов в тексте
        self.frqDict = dict()      # frequency dictionary частотный словарь
        self.setWords = set()      # множество для определения количества разных слов в тексте
        self.substrLen = substrLen

    # из txtArr в рабочий буфер приложения.
    # Форматирование буфера строк txtArr:
    # каждая строка из txtArr разбивается на
    # фрагменты размером substrLen. В конец каждого фрагмента,
    # если его длина равна substrLen записывается символ '\n'
    def txtFormat(self):

        for i in range(0, len(self.txtArr)):

            # строка текстового буфера txtArr
            strTxt = self.txtArr[i]

            # пропуск строк, которые состоят только из '\n'
            if strTxt == '\n':
                continue

            q = 0
            for j in range(0, len(strTxt)):
                # посимвольный перебор строки strTxt и
                # определение очередного места вставки '\n'
                # в self.strings
                self.strings = self.strings + strTxt[j]
                q += 1
                if q > 0 and q >= self.substrLen and strTxt[j] == ' ':
                    self.strings = self.strings + '\n'
                    q = 0

        print(self.strings)

    # удаление нежелательных символов
    def signDelete(self):

        buff = ''
        for i in range(0, len(self.strings)):
            signX = self.strings[i]
            for s in txtParser.badSignes:  # цикл по списку нежелательных символов
                if signX == s:  # нежелательный символ в строке
                    signX = ''  # заменяется пустым
                    break

            buff = buff + signX  # проверенные символы записываются в буфер
        self.strings = buff  # зачищенный буфер записывается в исходную строку

        print(self.strings)  # печать результата

    # привести все слова к нижнему регистру (map) с применением map ======
    def lowerRunner(self):

        # строка - список
        buff = self.strings.split()

        # перевод к нижнему регистру
        for i in range(len(buff)):
            buff[i] = buff[i].lower()

        # сборка строки из списка приведённых слов с применением функции map
        self.stringsLower = ' '.join(map(str, buff))

        print(self.stringsLower)

    # привести все слова к нижнему регистру (map) с применением map ======
    def lowerRunnerTxt(self):

        buff = ''
        for i in range(0, len(self.strings)):
            signX = self.strings[i].lower()
            buff = buff + signX   # приведённые символы записываются в буфер

        self.stringsLowerTxt = buff  # зачищенный буфер записывается в
                                     # буфер для приведения слов к нижнему регистру
        print(self.stringsLowerTxt)  # печать результата
"""

= Wordnet Lemmatizer        --> Лемматизатор Wordnet из NLTK
                                Wordnet – это большая, свободно распространяемая
                                и общедоступная лексическая база данных для 
                                английского языка с целью установления структурированных 
                                семантических отношений между словами. 
                                Библиотека также предлагает возможности лемматизации и является
                                одним из самых ранних и наиболее часто используемых лемматизаторов.
                                
= TextBlob                  --> TextBlob Lemmatizer – это мощный, быстрый пакет NLP. 
                                Используя объекты Word и TextBlob, довольно просто 
                                анализировать и лемматизировать слова и предложения 
                                соответственно. Чтобы лемматизировать предложение или абзац, 
                                он анализируется его с помощью TextBlob а затем вызывается функция
                                lemmatize() для проанализированных слов. Он не справляетчя с работой. 
                                Как и NLTK, TextBlob внутри использует Wordnet. Соотвественно, 
                                для корректной работы он так же требует передачу соответствующего 
                                POS-тега методу lemmatize(). TextBlob Lemmatizer примнняется
                                с соответствующим
                                POS-тегом
                               
= spaCy Lemmatizer         --> spaCy лемматизация является относительно новым пакетом
                               и на данный момент считается стандартом в индустрии NLP.
                               Он поставляется с предварительно созданными моделями, 
                               которые могут анализировать текст и выполнять различный 
                               функционал, связанный с NLP. 
                               
= CLiPS Pattern             --> Pattern by CLiPs (Pattern Lemmatizer) – это универсальный
                                пакет со многими возможностями NLP. позволяет просмотреть 
                                возможные лексемы для каждого слова. Также можно получить 
                                лемму, анализируя текст.
                                
= Stanford CoreNLP          --> Stanford CoreNLP – так же популярный (!!!) инструмент NLP, 
                                который изначально был реализован на Java. Это для Mac...
                                Также был сделано несколько враперов на Python. 
                                Он достаточно удобен (???) в использовании. Короче, НЕТ!

= Gensim Lemmatizer         --> Gensim Лемматизация. Предоставляет средства лемматизации 
                                на основе пакета pattern. Использует метод lemmatize() 
                                в модуле utils. По умолчанию Gensim lemmatize() 
                                допускает только теги «JJ», «VB», «NN» и «RB».
                                  
= TreeTagger                --> TreeTagger является POS таггером для многих языков. 
                                И он также предоставляем возможности лемматизации.
                                реализации НЕ будет...


__________________________________________________________________________
1. WordNet -> лексическая база данных из более чем 200 языков.
Обеспечивает семантические отношения между словами. 
Он присутствует в библиотеке nltk на python.
Wordnet связывает слова в семантические отношения (например, синонимы).
Он группирует синонимы в виде синтаксических наборов.
synsets : группа элементов данных, которые семантически эквивалентны. 
Один из самых ранних и наиболее часто используемых методов лемматизации.

Как использовать: 
- надо загрузить пакет nltk,
- импортировать nltk:
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

__________________________________________________________________________
2. Wordnet (с тегом POS) 
В приведенном выше подходе результаты Wordnet не всегда корректны. 
Такие слова, как "сидеть", "летать" и т. д., остались прежними после лемматизации. 
Это потому, что эти слова рассматриваются как существительное в данном предложении, а не глагол. 
Чтобы преодолеть это, мы используются теги POS (часть речи).
Этот тег добавляется с определенным словом, определяющим его тип 
(глагол, существительное, прилагательное и т. д.).
Например,
Word + Type (POS—тег) -> Лемматизированное слово
вождение + глагол 'v' —> водить
собак + существительное 'n' —> собака

--------------------------------------------------------------------------
3. Текстовый блок TextBlob
Это библиотека python, используемая для обработки текстовых данных. 
Он предоставляет простой API для доступа к своим методам и выполнения основных задач NLP.
Надо загрузить пакет TextBlob

--------------------------------------------------------------------------
4. Текстовый блок TextPOS
То же, что и в подходе Wordnet без использования соответствующих тегов POS, 
наблюдаются те же ограничения, что и в этом подходе. 
Чтобы преодолеть эту проблему, здесь используется один из наиболее мощных 
аспектов модуля TextBlob - пометка "Часть речи".

--------------------------------------------------------------------------

5. spaCy
Работа с библиотекой spaCy для выполнения еще нескольких основных задач НЛП, 
таких как токенизация , стемминг и лемматизация.
spaCy является относительно новым пакетом и на данный момент считается 
стандартом в индустрии NLP. Он поставляется с предварительно созданными моделями, 
которые могут анализировать текст и выполнять различный функционал, связанный с NLP.
spaCy по умолчанию определяет часть речи и назначает соответствующую лемму.

Библиотека spaCy - одна из самых применяемых библиотек NLP наряду с библиотекой NLTK. 
Основное различие между двумя библиотеками заключается в том, что NLTK содержит МНОГО 
алгоритмов для решения одной проблемы, а spaCy содержит только один, но ЛУЧШИЙ алгоритм
для решения проблемы.

NLTK был выпущен еще в 2001 году, spaCy был разработан в 2015 году. 
Далее при описании NLP основном будет применяться spaCy из-за его современного состояния. 
Но когда легче выполнить задачу, используя NLTK, а не spaCy, будет применяться NLTK.

Для работы с spaCy прежде всего (!!!) надо установить spaCy и загрузить модель «en».

# Install spaCy (run in terminal/prompt)
import sys

# Установка spaCy 
pip3 install -U spacy

# Загрузка модели для анализа английского языка
### python3 -m spacy download en_core_web_lg
python -m spacy download en 

# Установка textacy
pip3 install -U textacy

https://spacy.io/models/en
Это ЕДИНСТВЕННЫЙ (!!!) сайт, с которого удалось загрузить требуемые 
компоненты spaCy

После загрузки и установки spaCy следующим шагом будет загрузка языковой модели. 
Можно использовать одновременно множество моделей (англоязычную и русскоязычную) модель. 
Языковые модели используются для выполнения множества задач НЛП.

paCy, давайте вкратце посмотрим, как с ним работать.

Первый шаг - импортирт spacy библиотеку следующим образом:

import spacy 

Далее нужно загрузить языковую модель spaCy.

 sp = spacy.load('en_core_web_sm') 

Здесь используется функция load spacy библиотеки для загрузки базовой 
английской языковой модели. Модель хранится в переменной sp
Теперь создаётся документ и используется эта модель. 
Документ может быть предложением или группой предложений и может иметь неограниченную длину. 
Следующий скрипт создает простой документ spaCy.

sentence = sp(u'Manchester United is looking to sign a forward for $90 million') 

Когда документ создается с использованием модели, SpaCy автоматически разбивает входную строку
на токены. Токен просто относится к отдельной части предложения, имеющей некоторое 
семантическое значение. 

Результат выполнения сценария выше выглядит следующим образом:

 Manchester 
 United 
 is 
 looking 
 to 
 sign 
 a 
 forward 
 for 
 $ 
 90 
 million 
 
Таким образом, в документе есть следующие токены. 
Также, используя .pos_ можно увидеть части речи каждого из этих токенов:

 for word in sentence: 
 print(word.text, word.pos_) 
 
Вывод:

 Manchester PROPN 
 United PROPN 
 is VERB 
 looking VERB 
 to PART 
 sign VERB 
 a DET 
 forward NOUN 
 for ADP 
 $ SYM 
 90 NUM 
 million NUM 
 
Здесь каждому слову или символу в предложении была сопоставлена часть речи. 
Например, «Манчестер» был помечен как существительное собственное, 
«Смотрю» - как глагол и так далее.

Наконец, помимо частей речи, также можено видеть зависимости.

Ещё один документ:

 sentence2 = sp(u"Manchester United isn't looking to sign any forward.") 
 
Для синтаксического анализа зависимостей используется атрибут dep_ :

 for word in sentence2: 
 print(word.text, word.pos_, word.dep_) 
 
Результат:

 Manchester PROPN compound 
 United PROPN nsubj 
 is VERB aux 
 n't ADV neg 
 looking VERB ROOT 
 to PART aux 
 sign VERB xcomp 
 any DET advmod 
 forward ADV advmod 
 . PUNCT punct 

Из выходных данных видно, что spaCy может найти 
зависимость между токенами. Например, в данном предложении
есть слово is'nt. 
Синтаксический анализатор зависимостей разбил его на два слова и указывает, 
что n't на самом деле является отрицанием предыдущего слова.

Помимо печати слов, также можно распечатать предложения из документа.

document = sp(u'Hello from Stackabuse. The site with the best Python Tutorials. What are you looking for?') 

Используя следующий сценарий, можно перебирать каждое предложение:

 for sentence in document.sents: 
 print(sentence) 
 
Результат:

 Hello from Stackabuse. 
 The site with the best Python Tutorials. 
 What are you looking for? 
 
Можно проверить, начинается ли предложение с определенного токена или нет. 
Можно получить отдельные токены, используя индекс и квадратные скобки, как у массива:

document[4] 

В приведенном выше сценарии в документе ищется 5-е слово 
(здесь индекс начинается с нуля, а период (???) считается токеном). 
На выходе вы должно быть:

 The 

Теперь, чтобы увидеть, начинается ли какое-либо предложение в документе с The, 
можно использовать is_sent_start:

 document[4].is_sent_start 
 
Поскольку токен The используется в начале второго предложения, результатом будет True.

Выше были показаны несколько основных операций библиотеки spaCy. 
Далее будут рассмотрены токенизация, 
                            стемминг и 
                                лемматизация.

Токенизация
Токенизация - это процесс разбиения документа на слова, знаки препинания, числовые цифры и т. Д.

Токенизация spaCy как она есть. С использованием следующего скрипта создаётся новый документ:

 sentence3 = sp(u'"They\'re leaving UK for USA"') 
 print(sentence3) 

Предложение содержит кавычки в начале и в конце. 
Оно также содержит знаки препинания в сокращениях «UK» и «USA» (где ???).
Вот как spaCy токенизует это предложение.

 for word in sentence3: 
 print(word.text) 
 
Вывод:

 " 
 They 
 're 
 leaving 
 UK 
 for 
 USA 
 " 
В выходных данных можро увидеть, что spaCy разметила начальную и конечную двойные кавычки. 
Тем не менее, он не разметил точку пунктуации между аббревиатурами, такими как UK и USA.
Посмотрим еще один пример токенизации:

 sentence4 = sp(u"Hello, I am non-vegetarian, email me the menu at [email protected] ") 
 print(sentence4) 
 
В этом предложении есть тире в слове «не-вегетарианец» и в адресе электронной почты (где ???). 
Результат токениззации spaCy:

 for word in sentence4: 
 print(word.text) 
 
Вывод:

 Hello 
 , 
 I 
 am 
 non 
 - 
 vegetarian 
 , 
 email 
 me 
 the 
 menu 
 at 
 [email protected] 

Видно, что spaCy смог обнаружить электронное письмо и не токенизировал его, 
несмотря на наличие знака «-» (???). 
С другой стороны, слово «не-вегетарианец» было символическим.

Теперь о подсчёте слов в документе:

 len(sentence4) 
 
На выходе 14 - это количество токенов в sentence4 .

Обнаружение сущностей
Помимо токенизации документов в слова, можно также  узнать, 
является ли слово сущностью, такой как компания, место, здание, валюта, учреждение и т. Д.

Пример распознавания именованных сущностей:

 sentence5 = sp(u'Manchester United is looking to sign Harry Kane for $90 million') 
 
Попытка простой токенизации:

 for word in sentence5: 
 print(word.text) 
 
Вывод:

 Manchester 
 United 
 is 
 looking 
 to 
 sign 
 Harry 
 Kane 
 for 
 $ 
 90 
 million 

Извнстно, что «Манчестер Юнайтед» - это одно слово, поэтому его нельзя 
токенизировать двумя словами. 
Точно так же «Гарри Кейн» - это имя человека, а «90 миллионов долларов» - 
это денежная ценность. Их также не следует токенизировать.

Здесь начинает работать распознавание именованных сущностей. 
Чтобы получить именованные сущности из документа, надо использовать атрибут ents.
Извлечение названных сущностей из приведенного выше предложения. 
Выполняется следующий скрипт:

 for entity in sentence.ents: 
 print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_))) 

В приведенном выше скрипте печатается текст объекта, 
метка объекта и детали объекта. 

Вывод:

 Manchester United - ORG - Companies, agencies, institutions, etc. 
 Harry Kane - PERSON - People, including fictional 
 $90 million - MONEY - Monetary values, including unit 

Таким образом, средство распознавания именованных сущностей spaCy успешно распознало 
«Манчестер Юнайтед» как организацию, «Гарри Кейна» как личность и «90 миллионов долларов» 
как денежную ценность.

Обнаружение существительных
Помимо обнаружения именованных сущностей, также могут быть обнаружены существительные. 
Для этого  применяется атрибут noun_chunks. Следующее предложение:

 sentence5 = sp(u'Latest Rumours: Manchester United is looking to sign Harry Kane for $90 million') 

Поиск существительных из этого предложения:

 for noun in sentence5.noun_chunks: 
 print(noun.text)
  
Вывод:

 Latest Rumours 
 Manchester United 
 Harry Kane 
 
Видно, что существительное также может быть именованным объектом, и наоборот.

Стемминг
Основание относится к приведению слова к его корневой форме. 
При выполнении задач обработки естественного языка могутприменяться различные сценарии, 
в которых находятся разные слова с одним и тем же корнем. 
Например, compute, computer, computing, computed и т. Д. 
Для достижения единообразия можно уменьшить слова до их корневой формы. 
здесь применяется стемминг.
В spaCy нет (!!!) функции для стемминга, поскольку он полагается только на лемматизацию. 
Поэтому здесь для стемминга используется NLTK.
В NLTK есть два типа стеммеров: Porter Stemmer и Snowball . 
Оба они были реализованы с использованием разных алгоритмов.

Портер Стеммер
Стеммер портера в действии:

 import nltk 
 
 from nltk.stem.porter import * 
 
Создаётся класс PorterStemmer.

 stemmer = PorterStemmer()
  
Прусть имеется следующий список, и надо сократить эти слова до основы:

 tokens = ['compute', 'computer', 'computed', 'computing'] 
 
Следующий скрипт находит основу для слов в списке с помощью стеммера портера:

 for token in tokens: 
 print(token + ' --> ' + stemmer.stem(token)) 
 
Результат выглядит следующим образом:

 compute --> comput 
 computer --> comput 
 computed --> comput 
 computing --> comput 
 
Все 4 слова были сокращены до «вычислить», что на самом деле вовсе не слово.

"Снежок" Стеммер
Стеммер Snowball - это немного улучшенная версия стеммера Porter, 
которую обычно предпочитают последнему. Стеммер снежка в действии:

 from nltk.stem.snowball import SnowballStemmer 
 
 stemmer = SnowballStemmer(language='english') 
 
 tokens = ['compute', 'computer', 'computed', 'computing'] 
 
 for token in tokens: 
 print(token + ' --> ' + stemmer.stem(token)) 
 
В приведенном выше сценарии используется стеммер Snowball, чтобы найти основу тех же 4 слов, 
что и стеммер портера. 
Результат:

 compute --> comput 
 computer --> comput 
 computed --> comput 
 computing --> comput 
 
Это те же самые результаты. В качестве основы всё ещё существует «вычислитель» . 
Опять же, слово «вычислить» на самом деле не словарное.

Вот тут-то и нужна лемматизация. 
Лемматизация сокращает слово до основы, как оно появляется в словаре. 
Основы, возвращаемые посредством лемматизации, являются фактическими словарными словами 
и семантически полными, в отличие от слов, возвращаемых стеммером.

Лемматизация
Хотя и не получается выполнить стемминг с помощью spaCy, можно выполнить 
лемматизацию с помощью spaCy.

Для этого нужно использовать lemma_ атрибут на Spacy документа. 
Пусть дано следующее предложение:

 sentence6 = sp(u'compute computer computed computing') 

Можно найти корни всех слов с помощью лемматизации spaCy следующим образом:

 for word in sentence6: 
 print(word.text, word.lemma_) 

Результат выполнения сценария:

 compute compute 
 computer computer 
 computed compute 
 computing computing 

В отличие от стемминга, в котором корень, который был получен, «вычислялся», 
корни, которые были здесь получены, являются настоящими словами в словаре.

Лемматизация преобразует слова во второй или третьей формах в их варианты первой формы. 
Например:

 sentence7 = sp(u'A letter has been written, asking him to be released') 
 
 for word in sentence7: 
 print(word.text + ' ===>', word.lemma_) 

Вывод:

 A ===> a 
 letter ===> letter 
 has ===> have 
 been ===> be 
 written ===> write 
 , ===> , 
 asking ===> ask 
 him ===> -PRON- 
 to ===> to 
 be ===> be 
 released ===> release 

Из выходных данных видно, что слова во второй и третьей формах, 
такие как «написано», «выпущено» и т. д., были преобразованы в первую форму, 
то есть «запись» и «выпуск».

Заключение
Токенизация, стемминг и лемматизация - фундаментальные задачи обработки естественного языка. 
Здесь было показано, как можно выполнить токенизацию и лемматизацию с помощью библиотеки spaCy. 
Также было показано, как NLTK можно использовать для стемминга. 

"""


def WordNet(wnl, pars):
    # single word lemmatization examples
    # Array of words
    list0 = pars.stringsLower.split(' ')
    list1 = []
    for word in list0:
       list1.append(word)

    for words in list1:
        print(words + " ---> " + wnl.lemmatize(words))

    # ========================================================================
    # sentence lemmatization examples ========================================
    string = pars.stringsLowerTxt
    print(string)

    # Converting string into tokens ==========================================
    list2 = nltk.word_tokenize(string)
    print(list2)


    lemmatized_string = ' '.join([wnl.lemmatize(words) for words in list2])

    print(lemmatized_string)

# ========================================================================
def WordNetPos(pars):
    lemmatizer = WordNetLemmatizer()

    # Define function to lemmatize each word with its POS tag

    # POS_TAGGER_FUNCTION : TYPE 1
    def pos_tagger(nltk_tag):
        if nltk_tag.startswith('J'):
            return wordnet.ADJ
        elif nltk_tag.startswith('V'):
            return wordnet.VERB
        elif nltk_tag.startswith('N'):
            return wordnet.NOUN
        elif nltk_tag.startswith('R'):
            return wordnet.ADV
        else:
            return None

    sentence = pars.stringsLower
    # tokenize the sentence and find the POS tag for each token
    pos_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))

    print(pos_tagged)

    # As you may have noticed, the above pos tags are a little confusing.
    # we use our own pos_tagger function to make things simpler to understand.

    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))
    print(wordnet_tagged)


    lemmatized_sentence = []
    for word, tag in wordnet_tagged:
        if tag is None:
            # if there is no available tag, append the token as is
            lemmatized_sentence.append(word)
        else:
            # else use the tag to lemmatize the token
            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))

    lemmatized_sentence = " ".join(lemmatized_sentence)

    print(lemmatized_sentence)


def WordTextBlob(pars):

    sentence = pars.stringsLowerTxt

    # сборка строки из результатов работы лемматизатора
    s = TextBlob(sentence)
    lemmatized_sentence = " ".join([w.lemmatize() for w in s.words])

    print(lemmatized_sentence)
    # > the bat saw the cat with stripe hanging upside down by their foot


# ========================================================================

def getWordNetPos(sentence):
    """
    Map POS tag to first character lemmatize() accepts
    Wordnet Lemmatizer с соответствующим POS-тегом
    Достаточно сложно вручную проставить соответствующий POS-тег для каждого слова
    при обработке больших текстов.
    Поэтому вместо этого находят правильный POS-тег для каждого слова,
    сопоставляют его с правильным входным символом, который принимает WordnetLemmatizer,
    и передают его в качестве второго аргумента в lemmatize().

    Как получить POS-тег для выбранного слова?
    В nltk для этого есть метод nltk.pos_tag().
    Он принимает только список (список слов), даже если нужно передать только одно слово.
    """
    tag = nltk.pos_tag([sentence])[0][1][0].upper()
    # это выше моего понимания !!!

    # здесь принципиально сопоставление POS-тегов NLTK с форматом,
    # принятым лемматизатором wordnet
    # здесь без вариантов: в словаре только один элемент.
    # Закомментированные элементы словаря не используются
    # и воообще, похоже, что конкретный словарь БЕЗ разницы
    # tag_dict = {"V": wordnet.VERB}
    tag_dict = {"N": wordnet.NOUN}

    # tag_dict = { "J": wordnet.ADJ,
    #              "N": wordnet.NOUN,
    #              "V": wordnet.VERB,
    #              "R": wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)

def WordTextPos(wnl, pars):

    #sentence = 'the bat saw the cat with stripe hang upside down by their foot'
    sentence = pars.stringsLower
    print(sentence)

    # resSentence = [wnl.lemmatize(w, getWordNetPos(w)) for w in nltk.word_tokenize(sentence)]
    #                                 ^^^^^^^^^^^^^  правильный POS-тег для каждого слова в sentence
    #                               второй аргумент в lemmatize() - это результат getWordNetPos
    #                            правильный POS-тег для каждого слова,
    #                   lemmatize с двумя аргументами занимается СОПОСТАВЛЕНИЕМ
    #             WordnetLemmatizer принимает результат сопоставления и возвращает массив
    #  _________________________ И вся эта хрень в одну строку! ____________________________________

    # _____________ А вот то же самое в две строки. Легче стало? ___________________________________
    resSentence = []
    for w in nltk.word_tokenize(sentence):
        wnp = getWordNetPos(w)
        resSentence.append(wnl.lemmatize(w, wnp))

    print(resSentence)

# ========================================================================
"""
# Install spaCy (run in terminal/prompt)
import sys

# Установка spaCy 
pip3 install -U spacy

# Загрузка модели для анализа английского языка
python3 -m spacy download en_core_web_lg

# Установка textacy
pip3 install -U textacy

spaCy по умолчанию определяет часть речи и назначает соответствующую лемму.
"""

def xspaCy(pars):
    # Загрузка английской или русской NLP-модели

    # sp = spacy.load("ru_core_news_lg") # ??? грузится,но не работает
    # sp = spacy.load("en_core_web_sm")  # а так он анализирует как en, так и ru
    # sp = spacy.load("en_core_web_lg")  # а так он анализирует как en, так и ru
    # sp = spacy.load("en_core_web_trf")  # а так он анализирует ТОЛЬКО en

    sp = spacy.load("en_core_web_md")  # а так он анализирует как en, так и ru,
                                       # и даже лучше, чем sm

    # используется функция load spacy библиотеки для загрузки базовой (возможно, английской)
    # языковой модели. Модель хранится в переменной sp
    # # Initialize spacy 'en' model, keeping only tagger component needed for lemmatization

    # Текст для анализа - сейчас он берётся от объекта .........
    # text = """London is the capital and most populous city of England and
    # the United Kingdom.  Standing on the River Thames in the south east
    # of the island of Great Britain, London has been a major settlement
    # for two millennia. It was founded by the Romans, who named it Londinium.
    # spaCy является относительно новым пакетом и на данный момент считается
    # стандартом в индустрии NLP is'nt. Он поставляется с предварительно созданными моделями,
    # которые могут анализировать текст и выполнять различный функционал, связанный с NLP.
    # Прежде всего надо установить spaCy и загрузить модель «en».
    # """

    text = pars.stringsLowerTxt

    # Парсинг текста с помощью spaCy. Эта команда запускает конвейер
    doc = sp(text)
    # в переменной 'doc' теперь содержится обработанная версия текста
    # когда документ создается с использованием модели.
    # SpaCy автоматически разбивает документ text на токены.
    # Токен относится к отдельной части предложения, имеющей некоторое семантическое значение.

    # Можно посмотреть, какие токены есть в документе.
    for word in doc:
        print(word.text)

    print('\n__________________________________________________________')

    # можно распечатать все обнаруженные именованные сущности
    for entity in doc.ents:
        print(f"{entity.text} ({entity.label_})")

    print('\n__________________________________________________________')

    # используя .pos_ показанный ниже, также можно посмотреть части речи каждого из этих токенов
    for word in doc:
        print(word.text, word.pos_)

    print('\n__________________________________________________________')

    # видеть, что каждому слову или символу в нашем предложении была отведена часть речи.
    # Например, «London» был помечен как существительное собственное, «is» - как глагол
    # и так далее

    # Помимо частей речи, также можем видеть зависимости.
    for word in doc:
        print(word.text, word.pos_, word.dep_)

    print('\n__________________________________________________________')

    # Из выходных данных видно, что spaCy может найти зависимость между токенами,
    # например, в предложении, которое у нас есть, есть слово is'nt .
    # Синтаксический анализатор зависимостей разбил его на два слова и указывает,
    # что n't на самом деле является отрицанием предыдущего слова.
    # Помимо печати слов, также можно распечатать предложения из документа.

    for sentence in doc.sents:
        print(sentence)

    print('\n__________________________________________________________')

    # также можно проверить, начинается ли предложение с определенного токена или нет.
    # Можно получить отдельные токены, используя индекс и квадратные скобки, как массив:

    print(doc[5])

    # В приведенном выше сценарии мы ищется 6-е слово в документе.
    # Надо иметь в виду, что индекс начинается с нуля, а период(???) считается токеном.
    # На выходе: .......

    # Теперь, чтобы увидеть, начинается ли какое-либо предложение в документе с
    # populous, можно использовать is_sent_start как показано ниже:

    if doc[5].is_sent_start == True:
        print("Yes!")
    else:
        print("No!")

    print('\n__________________________________________________________')

    # токенизация spaCy. С использованием следующего скрипта создаётся новый документ:

    doc = sp(u'"They\'re leaving UK for USA"')
    print(doc)

    print('\n__________________________________________________________')

    # Предложение содержит кавычки в начале и в конце.
    # Оно также содержит знаки препинания в сокращениях «UK» и «USA».

    for word in doc:
        print(word.text)

    print('\n__________________________________________________________')

    # В выходных данных можно видеть, что spaCy разметила начальную и
    # конечную двойные кавычки.
    # И он НЕ разметил точку пунктуации между аббревиатурами UK и USA.

    # Ещё один пример токенизации:

    doc = sp(u"Hello, I am non-vegetarian, email me the menu at [email protected] ")
    print(doc)

    for word in doc:
        print(word.text)

    print('\n__________________________________________________________')

    # Как можно подсчитать слова в документе:

    print(len(doc))

    # Обнаружение сущностей ==============================================
    # Помимо токенизации документов в слова, также можно узнать,
    # является ли слово сущностью, такой как компания, место, здание, валюта, учреждение и т. д.
    # Пример распознавания именованных сущностей:

    doc = sp(u'Manchester United is looking to sign Harry Kane for $90 million')

    # Простая токенизация:

    for word in doc:
        print(word.text)

    print('\n__________________________________________________________')

    # Известно, что «Манчестер Юнайтед» - это одно слово, поэтому его нельзя
    # токенизировать двумя словами. Точно так же «Гарри Кейн» - это имя человека,
    # а «90 миллионов долларов» - это денежная ценность. Их также не следует токенизировать.
    # Здесь и применяется распознавание именованных сущностей.
    # Чтобы получить именованные сущности из документа, надо использовать атрибут ents.
    # Теперь извлечние названных сущностей из приведенного выше предложения.
    # Надо вВыполнить следующий скрипт:

    for word in doc.ents:
        print(word.text + ' - ' + word.label_ + ' - ' + str(spacy.explain(word.label_)))

    print('\n__________________________________________________________')

    # В приведенном выше скрипте печатается текст объекта, метка объекта и детали объекта.
    # Средство распознавания именованных сущностей spaCy успешно распознало
    # «Манчестер Юнайтед» как организацию, «Гарри Кейна» как личность и
    # «90 миллионов долларов» как денежную ценность.

    # Обнаружение существительных ========================================
    # Помимо обнаружения именованных сущностей, также могут быть обнаружены
    # существительные. Для этого применяется атрибут noun_chunks:

    doc = sp(u'Latest Rumours: Manchester United is looking to sign Harry Kane for $90 million')

    # найти существительные из этого предложения (существительное также может быть именованным объектом, и наоборот.):

    for word in doc.noun_chunks:
        print(word.text)

    print('\n__________________________________________________________')

    # Стемминг ===========================================================
    # Основание относится к приведению слова к его корневой форме.
    # При выполнении задач обработки естественного языка вы столкнетесь с
    # различными сценариями, в которых можно найти разные слова с одним и тем же корнем.
    # Например, compute, computer, computing, computed и т. Д.
    # Можно уменьшить слова до их корневой формы для единообразия.
    # И здесь работает стемминг.
    # spaCy применяет только лемматизацию и в нём нет функции для стемминга.
    # Поэтому для стемминга используется NLTK.
    #
    # В NLTK есть два типа стеммеров: Porter Stemmer и Snowball.
    # Оба они были реализованы с использованием разных алгоритмов.
    #
    # Портер Стеммер
    #
    # import nltk  # !!!!!
    # from nltk.stem.porter import *
    # Создаётся класс PorterStemmer.

    stemmer = PorterStemmer()

    # Пусть имеется следующий список, и надо сократить эти слова до основы:

    tokens = ['compute', 'computer', 'computed', 'computing']

    # Следующий скрипт находит основу для слов в списке с помощью стеммера портера:

    for token in tokens:
        print(token + ' --> ' + stemmer.stem(token))

    print('\n__________________________________________________________')

    # Результат выглядит следующим образом:
    #
    #  compute --> comput
    #  computer --> comput
    #  computed --> comput
    #  computing --> comput

    # Видно, что все 4 слова были сокращены до «вычислить», что на самом деле вовсе не слово.
    #
    # Снежок Стеммер # ===================================================
    # Стеммер Snowball - это немного улучшенная версия стеммера Porter,
    # которую обычно предпочитают последнему. Стеммер снежка в действии:
    #
    # from nltk.stem.snowball import SnowballStemmer

    stemmer = SnowballStemmer(language='english')

    tokens = ['compute', 'computer', 'computed', 'computing']

    for token in tokens:
        print(token + ' --> ' + stemmer.stem(token))

    print('\n__________________________________________________________')

    # В приведенном сценарии используется стеммер Snowball,
    # чтобы найти основу тех же 4 слов, что и стеммер портера. Результат выглядит так:
    #
    #  compute --> comput
    #  computer --> comput
    #  computed --> comput
    #  computing --> comput

    # Можно видеть, что результаты такие же. Всё ещё есть «вычислитель» в качестве основы.
    # Опять же, слово «вычислить» на самом деле не словарное.
    #
    # Здесь нужна лемматизация. Лемматизация сокращает слово до основы,
    # как оно появляется в словаре. Основы, возвращаемые посредством лемматизации,
    # являются фактическими словарными словами и семантически полными,
    # в отличие от слов, возвращаемых стеммером.
    #
    # Лемматизация # =====================================================
    # Хотя и не возможно выполнить стемминг с помощью spaCy напрямую, всё же
    # можно выполнить эту лемматизацию с помощью spaCy.
    #
    # Для этого нам нужно использовать lemma_ атрибут на Spacy документа.
    # Пусть, есть следующее предложение:

    sentence6 = sp(u'compute computer computed computing')
    # Можно найти корни всех слов с помощью лемматизации spaCy следующим образом:

    for word in sentence6:
        print(word.text, word.lemma_)

    print('\n__________________________________________________________')

    # Результат выполнения сценария выше выглядит следующим образом:
    #
    #  compute compute
    #  computer computer
    #  computed compute
    #  computing computing

    # Можно видеть, что в отличие от стемминга, в котором корень,
    # который мы получается, был «вычислить», корни, которые мы здесь получили,
    # являются настоящими словами в словаре.
    #
    # Лемматизация преобразует слова во второй или третьей формах в их варианты первой формы.
    # Ещё один пример:
    #
    sentence7 = sp(u'A letter has been written, asking him to be released')

    for word in sentence7:
        print(word.text + ' ===>', word.lemma_)

    print('\n__________________________________________________________')

    # Вывод: ???
    #
    #  A ===> a
    #  letter ===> letter
    #  has ===> have
    #  been ===> be
    #  written ===> write
    #  , ===> ,
    #  asking ===> ask
    #  him ===> -PRON-
    #  to ===> to
    #  be ===> be
    #  released ===> release

    # Из выходных данных видно, что слова во второй и третьей формах,
    # такие как «написано», «выпущено» и т. Д.,
    # Были преобразованы в первую форму, то есть «запись» и «выпуск».
    #
    # Заключение # =======================================================
    # Токенизация, стемминг и лемматизация - одни из самых важных
    # задач обработки естественного языка. Здесь было показано,
    # как можно выполнить токенизацию и лемматизацию с помощью библиотеки spaCy.
    # Также было показано, как NLTK можно использовать для стемминга.

    # ========================================================================



def DoIt(name, substrlen):  # Create WordNetLemmatizer object
    print(name)

    pars = txtParser(name)

    print('\n*****txtFormat*************************************************************')
    pars.txtFormat()
    print('\n*****signDelete************************************************************')
    pars.signDelete()
    print('\n*****lowerRunner***********************************************************')
    pars.lowerRunner()
    print('\n*****lowerRunnerTxt********************************************************')
    pars.lowerRunnerTxt()

    wnl = WordNetLemmatizer()

    print(f'\n___{WordNet}___')
    WordNet(wnl, pars)

    print(f'\n___{WordNet}___')
    WordNetPos(pars)

    print(f'\n___{WordTextBlob}___')
    WordTextBlob(pars)

    print(f'\n___{WordTextPos}___')
    WordTextPos(wnl, pars)

    print(f'\n___{xspaCy}___')
    xspaCy(pars)


def main(name, substrlen):
    DoIt(name, substrlen)


# Press the green button in the gutter to run the script.
if __name__ == '__main__':
    main("C:\\PythonDrom\\Texts_2022\\InputDate.txt", 90)


